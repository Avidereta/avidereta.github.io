[{"authors":null,"categories":null,"content":"I am a PhD student at ETH Zurich in the Photogrammetry and Remote Sensing Lab, supervised by Prof. Dr. Konrad Schindler.\nBefore moving to Switzerland I completed my masters in Moscow where I graduated with honors from Moscow Institute of Physics and Technology (MIPT) and from Skolkovo Institute of Science and Technology (Skoltech)\nI started my expansion towards west with move to Kazan (where I spent 1 year at Innopolis) from Ekaterinburg (where I grew up and made my first bachelor degree in electrical engineering and graduated with honors from Ural Federal University).\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a PhD student at ETH Zurich in the Photogrammetry and Remote Sensing Lab, supervised by Prof. Dr. Konrad Schindler.\nBefore moving to Switzerland I completed my masters in Moscow where I graduated with honors from Moscow Institute of Physics and Technology (MIPT) and from Skolkovo Institute of Science and Technology (Skoltech)","tags":null,"title":"Mikhail Usvyatsov","type":"authors"},{"authors":["Shengyu Huang","Zan Gojcic","Mikhail Usvyatsov","Andreas Wieser","Konrad Schindler"],"categories":null,"content":"","date":1616112000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616112000,"objectID":"d3ff59608450b3e63974ec39eacd6236","permalink":"/publication/huang-2021-predator/","publishdate":"2021-03-19T01:18:36.552293Z","relpermalink":"/publication/huang-2021-predator/","section":"publication","summary":"A model for pairwise point-cloud registration with deep attention to the overlap region.","tags":null,"title":"PREDATOR: Registration of 3D Point Clouds with Low Overlap","type":"publication"},{"authors":[],"categories":[],"content":"I backed Keychron K3 on Kickstarter and after a long wait, I received the keyboard with brown optical switches (K3E3 modification) which is pretty awesome except for one nasty tiny detail: the keystroke registers far before the tactile bump.\nHere is the illustration of the fixing method which was also described on reddit.\nThe first switch took me some time to understand how it works. It is not very difficult to disassemble, but it\u0026rsquo;s easy to lose some parts. So before the start, I highly recommend palcing the patient switch inside a highlighted cardboard box and keep it there while working on it. Seriously, consider work inside the cardboard box. You might lose very tinny parts like spring or stem otherwise, they can jump away easily and the box will help to contain them.   Keychron brown optical switch \n  Take a look at the bottom of the switch:   Bottom of K3E3 switch  The red circle shows where the stem which breaks the light beam when the key is pressed. We will remove it. You don\u0026rsquo;t need to open the switch yet.\n  Press the switch.   Pressed K3E3 switch  The stem will move closer to the bottom of the switch.\n  Use tweezers or another appropriate tool to press the stem out of the housing.   Top of K3E3 switch    Taking the stem out  While taking the stem out keep the switch in your hand and make sure that the stem will fall out in a box / safe place.\n  Keep the stem in a safe place for a moment.   Stem of K3E3    Stem and spring separated \n  Now use tweezers to open the switch.   Opening of K3E3    Opened of K3E3  Don\u0026rsquo;t move the spring that you see in the open switch, they need to be left in their places. The big spring is creating the actuation force. The small spring is responsible for creating a tactile bump.\n  Now take the stem without the spring and insert from the bottom of the switch.   Inserting the stem into K3E3    Opened of K3E3  Then place the small spring from the inner side of the switch bottom.\n  Now pre-insert the brown part back on the place, but do not press fully yet, we need to move the tactile spring to a correct position.   Preinsertion    Placing the tactile spring    Full press    Closing  When fully pressed and tactile spring on place secure the stem in the brown part pressing from the bottom side with tweezers. And finally, close the switch fully. The upgrade is finished.\n  This post was typed on the upgraded K3E3. The fix took about 4 hours and improved typing speed by a factor of two and reduced the error rate by a factor of 6.\n","date":1614248045,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614248045,"objectID":"80783de95f082d2b8e277fac43f4d204","permalink":"/post/k3e3_fix/","publishdate":"2021-02-25T11:14:05+01:00","relpermalink":"/post/k3e3_fix/","section":"post","summary":"How I fixed Keychron K3 brown switch.","tags":[],"title":"Fixing Keychron K3 brown switch","type":"post"},{"authors":["Shengyu Huang","Mikhail Usvyatsov","Konrad Schindler"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"113b03cdfd16c2f2017761cefa13647c","permalink":"/publication/huang-2020-indoor/","publishdate":"2020-02-29T01:18:36.55387Z","relpermalink":"/publication/huang-2020-indoor/","section":"publication","summary":"Recognising in what type of environment one is located is an important perception task. For instance, for a robot operating in indoors it is helpful to be aware whether it is in a kitchen, a hallway or a bedroom. Existing approaches attempt to classify the scene based on 2D images or 2.5D range images. Here, we study scene recognition from 3D point cloud (or voxel) data, and show that it greatly outperforms methods based on 2D birds-eye views. Moreover, we advocate multi-task learning as a way of improving scene recognition, building on the fact that the scene type is highly correlated with the objects in the scene, and therefore with its semantic segmentation into different object classes. In a series of ablation studies, we show that successful scene recognition is not just the recognition of individual objects unique to some scene type (such as a bathtub), but depends on several different cues, including coarse 3D geometry, colour, and the (implicit) distribution of object categories. Moreover, we demonstrate that surprisingly sparse 3D data is sufficient to classify indoor scenes with good accuracy.","tags":null,"title":"Indoor Scene Recognition in 3D","type":"publication"},{"authors":["Mikhail Usvyatsov","Konrad Schindler"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"c319fab7656d65c1d8e4028ca51310c0","permalink":"/publication/usvyatsov-2019-visual/","publishdate":"2020-02-29T01:18:36.55387Z","relpermalink":"/publication/usvyatsov-2019-visual/","section":"publication","summary":"Recognising relevant objects or object states in its environment is a basic capability for an autonomous robot. The dominant approach to object recognition in images and range images is classification by supervised machine learning, nowadays mostly with deep convolutional neural networks (CNNs). This works well for target classes whose variability can be completely covered with training examples. However, a robot moving in the wild, i.e., in an environment that is not known at the time the recognition system is trained, will often face \u001bmph{domain shift}: the training data cannot be assumed to exhaustively cover all the within-class variability that will be encountered in the test data. In that situation, learning is in principle possible, since the training set does capture the defining properties, respectively dissimilarities, of the target classes. But directly training a CNN to predict class probabilities is prone to overfitting to irrelevant correlations between the class labels and the specific subset of the target class that is represented in the training set. We explore the idea to instead learn a Siamese CNN that acts as similarity function between pairs of training examples. Class predictions are then obtained by measuring the similarities between a new test instance and the training samples. We show that the CNN embedding correctly recovers the relative similarities to arbitrary class exemplars in the training set. And that therefore few, randomly picked training exemplars are sufficient to achieve good predictions, making the procedure efficient.","tags":null,"title":"Visual recognition in the wild by sampling deep similarity functions","type":"publication"},{"authors":["Timo Hackel","Mikhail Usvyatsov","Silvano Galliani","Jan D Wegner","Konrad Schindler"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"2a801d6f57e604d800ce6c841371d88d","permalink":"/publication/hackel-2018-inference/","publishdate":"2020-02-29T01:18:36.553322Z","relpermalink":"/publication/hackel-2018-inference/","section":"publication","summary":"While CNNs naturally lend themselves to densely sampled data, and sophisticated implementations are available, they lack the ability to efficiently process sparse data. In this work we introduce a suite of tools that exploit sparsity in both the feature maps and the filter weights, and thereby allow for significantly lower memory footprints and computation times than the conventional dense framework when processing data with a high degree of sparsity. Our scheme provides (i) an efficient GPU implementation of a convolution layer based on direct, sparse convolution; (ii) a filter step within the convolution layer, which we call attention, that prevents fill-in, i.e., the tendency of convolution to rapidly decrease sparsity, and guarantees an upper bound on the computational resources; and (iii) an adaptation of the back-propagation algorithm, which makes it possible to combine our approach with standard learning frameworks, while still exploiting sparsity in the data and the model.","tags":null,"title":"Inference, learning and attention mechanisms that exploit and preserve sparsity in convolutional networks","type":"publication"},{"authors":["Maxim Borisyak","Mikhail Usvyatsov","Michael Mulhearn","Chase Shimmin","Andrey Ustyuzhanin"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"2b883d6927bfb343ecb54d9784abf3a6","permalink":"/publication/borisyak-2017-muon/","publishdate":"2020-02-29T01:18:36.552293Z","relpermalink":"/publication/borisyak-2017-muon/","section":"publication","summary":"Neural architecture that allows for simultaneous optimization of computational cost with per-pixel cross-entropy loss.","tags":null,"title":"Muon trigger for mobile phones","type":"publication"}]